{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. use numpy to finish a computation\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "N, D = 3, 4\n",
    "\n",
    "# create numpy array\n",
    "x = np.random.randn(N, D) # standard normal distribution\n",
    "y = np.random.randn(N, D)\n",
    "z = np.random.randn(N, D)\n",
    "\n",
    "# c = (x*y+z)*1\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = np.sum(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.7170085378000675"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. use tensor to finish a computation\n",
    "import torch as t\n",
    "\n",
    "tx = t.from_numpy(x)\n",
    "# or you can use t.randn(N, D, requires_grad=True) to create tensor\n",
    "ty = t.from_numpy(y)\n",
    "tz = t.from_numpy(z)\n",
    "\n",
    "tx.requires_grad = True # if false then cannot calculate gradient\n",
    "\n",
    "ta = tx * ty\n",
    "tb = ta + tz\n",
    "tc = t.sum(tb)\n",
    "\n",
    "tc.item() # get value from scalar object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.76103773  0.12167502  0.44386323  0.33367433]\n",
      " [ 1.49407907 -0.20515826  0.3130677  -0.85409574]\n",
      " [-2.55298982  0.6536186   0.8644362  -0.74216502]]\n"
     ]
    }
   ],
   "source": [
    "# 3. use numpy to calculate gradient\n",
    "grad_c = 1.0\n",
    "grad_b = grad_c * np.ones((N, D))\n",
    "grad_a = grad_b.copy()\n",
    "grad_z = grad_b.copy()\n",
    "grad_x = grad_a * y\n",
    "grad_y = grad_a * x\n",
    "\n",
    "print(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7610,  0.1217,  0.4439,  0.3337],\n",
      "        [ 1.4941, -0.2052,  0.3131, -0.8541],\n",
      "        [-2.5530,  0.6536,  0.8644, -0.7422]], dtype=torch.float64)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 4. use autograd in pytorch to calculate gradient\n",
    "tc.backward()\n",
    "print(tx.grad)\n",
    "print(ty.grad) # none because yt.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(3.)\n",
      "torch.Size([])\n",
      "cpu\n",
      "None\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "# 5. about tensor\n",
    "t1 = t.tensor(3.0, requires_grad = True)\n",
    "\n",
    "print(t1.requires_grad)\n",
    "print(t1.data)\n",
    "print(t1.shape)\n",
    "print(t1.device)\n",
    "print(t1.grad_fn)\n",
    "print(t1.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7641,  0.4002,  0.9787,  2.2409],\n",
      "        [ 1.8676, -0.9773,  0.9501, -0.1514],\n",
      "        [-0.1032,  0.4106,  0.1440,  1.4543]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "tensor([[[ 1.7641,  0.4002,  0.9787,  2.2409],\n",
      "         [ 1.8676, -0.9773,  0.9501, -0.1514],\n",
      "         [-0.1032,  0.4106,  0.1440,  1.4543]]], dtype=torch.float64,\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(tx)\n",
    "# add one dimension by [None]\n",
    "print(tx[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = t.ones(3, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5., 5., 5., 5.],\n",
      "        [5., 5., 5., 5., 5.],\n",
      "        [5., 5., 5., 5., 5.]])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(img_t.sum(-2))\n",
    "print(img_t.mean(-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2000, 0.7000, 0.1000])\n",
      "tensor([[[0.2000]],\n",
      "\n",
      "        [[0.7000]],\n",
      "\n",
      "        [[0.1000]]])\n"
     ]
    }
   ],
   "source": [
    "weights = t.tensor([0.2, 0.7, 0.1])\n",
    "print(weights)\n",
    "# change dimension\n",
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)\n",
    "print(unsqueezed_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]), torch.Size([3, 5, 5]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueezed_weights.shape, img_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]],\n",
       "\n",
       "        [[0.7000, 0.7000, 0.7000, 0.7000, 0.7000],\n",
       "         [0.7000, 0.7000, 0.7000, 0.7000, 0.7000],\n",
       "         [0.7000, 0.7000, 0.7000, 0.7000, 0.7000],\n",
       "         [0.7000, 0.7000, 0.7000, 0.7000, 0.7000],\n",
       "         [0.7000, 0.7000, 0.7000, 0.7000, 0.7000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
       "         [0.1000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
       "         [0.1000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
       "         [0.1000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
       "         [0.1000, 0.1000, 0.1000, 0.1000, 0.1000]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t* unsqueezed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(853, 1280, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "img_arr = imageio.imread('./data/cat.jpg') # saved in numpy\n",
    "img_arr.shape # (Height, Width, Channel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 853, 1280])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = t.from_numpy(img_arr)\n",
    "# change to (Channel, Height, Width)\n",
    "out = img.permute(2, 0, 1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use auto_gradient for gradient decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data simulation\n",
    "# y=ax+b, a=5, b=10\n",
    "x = np.array([1.2, 4, 3.5, 17.3, 2.5, 34.9, 44.2, 14.2, 5, 9.1])\n",
    "# Simulate gaussian error with mean 0 and variance 2\n",
    "error = np.random.normal(0, 2, 10)\n",
    "a = 5\n",
    "b = 10\n",
    "y = a*x + b + error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500, loss: 25.134116\n",
      "Epoch: 1000, loss: 11.383090\n",
      "Epoch: 1500, loss: 6.480441\n",
      "Epoch: 2000, loss: 4.732525\n",
      "Epoch: 2500, loss: 4.109334\n",
      "Epoch: 3000, loss: 3.887142\n",
      "Epoch: 3500, loss: 3.807925\n",
      "Epoch: 4000, loss: 3.779682\n",
      "Epoch: 4500, loss: 3.769612\n",
      "Epoch: 5000, loss: 3.766022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 4.9135, 10.8572], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t = t.from_numpy(x)\n",
    "y_t = t.from_numpy(y)\n",
    "params = t.tensor([1.0, 0.0], requires_grad=True)\n",
    "epoch = 5000\n",
    "# first try 1e-2, and loss tends to be inf, so we use a smaller learning rate\n",
    "learning = 1e-3\n",
    "\n",
    "# Define Model\n",
    "def model(in_x, k, b):\n",
    "    return in_x * k + b\n",
    "\n",
    "# Define loss function\n",
    "def loss_fc(fit_y, real_y):\n",
    "    return ((fit_y - real_y)**2).mean()\n",
    "\n",
    "def training_loop(epoch, learning_rate, in_x, params, real_y):\n",
    "    for it in range(1, epoch+1):\n",
    "        # auto_grad is accumulative, so we need to set it to 0 at each iteration\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "            \n",
    "        fit_y = model(in_x, *params)\n",
    "        loss = loss_fc(fit_y, real_y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # update params without changing its options\n",
    "        # without this with block, params won't be updated\n",
    "        # this step could be done by optim (below)\n",
    "        with t.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "        \n",
    "        if it % 500 == 0:\n",
    "            print('Epoch: %d, loss: %f' % (it, float(loss)))\n",
    "    return params\n",
    "\n",
    "training_loop(epoch, learning, x_t, params, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500, loss: 3.764033\n",
      "Epoch: 1000, loss: 3.764033\n",
      "Epoch: 1500, loss: 3.764033\n",
      "Epoch: 2000, loss: 3.764033\n",
      "Epoch: 2500, loss: 3.764033\n",
      "Epoch: 3000, loss: 3.764033\n",
      "Epoch: 3500, loss: 3.764033\n",
      "Epoch: 4000, loss: 3.764033\n",
      "Epoch: 4500, loss: 3.764033\n",
      "Epoch: 5000, loss: 3.764033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 4.9113, 10.9188], requires_grad=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use optimizer in pyTorch\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD([params], lr=learning)\n",
    "\n",
    "def training_loop(epoch, optimizer, params, in_x, real_y):\n",
    "    for it in range(1, epoch+1):\n",
    "        fit_y = model(in_x, *params)\n",
    "        loss = loss_fc(fit_y, real_y)\n",
    "        \n",
    "        # set grad to zero if it is not\n",
    "        # this could be put anywhere before the backward\n",
    "        optimizer.zero_grad()\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # update\n",
    "        optimizer.step()\n",
    "        \n",
    "        if it % 500 == 0:\n",
    "            print('Epoch: %d, loss: %f' % (it, float(loss)))\n",
    "    return params\n",
    "\n",
    "training_loop(epoch, optimizer, params, x_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and validation set\n",
    "n_validation = int(0.2 * x_t.shape[0])\n",
    "\n",
    "# return a permutation of input data\n",
    "shuffled_indices = t.randperm(x_t.shape[0])\n",
    "\n",
    "# get train index and validation index\n",
    "train_indices = shuffled_indices[:-n_validation]\n",
    "val_indices = shuffled_indices[-n_validation:]\n",
    "\n",
    "# train data and validation data\n",
    "train_x = x_t[train_indices]\n",
    "va_x = x_t[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 0, 1, 4, 9, 6, 8, 5])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500, train_loss: 27.758680, validation_loss: 13.063032\n",
      "Epoch: 1000, train_loss: 12.295135, validation_loss: 3.857742\n",
      "Epoch: 1500, train_loss: 7.044099, validation_loss: 1.464773\n",
      "Epoch: 2000, train_loss: 5.260950, validation_loss: 1.079283\n",
      "Epoch: 2500, train_loss: 4.655437, validation_loss: 1.197261\n",
      "Epoch: 3000, train_loss: 4.449834, validation_loss: 1.382395\n",
      "Epoch: 3500, train_loss: 4.380020, validation_loss: 1.529747\n",
      "Epoch: 4000, train_loss: 4.356301, validation_loss: 1.629048\n",
      "Epoch: 4500, train_loss: 4.348251, validation_loss: 1.691455\n",
      "Epoch: 5000, train_loss: 4.345523, validation_loss: 1.729375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[4.9155]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([11.0975], requires_grad=True))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writing a loop using nn module\n",
    "import torch.nn as nn\n",
    "\n",
    "linear_model = nn.Linear(1,1) # 1: input size, 1: output size\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=1e-3)\n",
    "\n",
    "# we need to transform input and output into B*Nin\n",
    "# For linear regression, B is the number of rows, Nin is the features in the column\n",
    "\n",
    "train_x = x_t.unsqueeze(1)[train_indices].float() # change to float from double\n",
    "train_y = y_t.unsqueeze(1)[train_indices].float() # float64 is double in pytorch\n",
    "va_x = x_t.unsqueeze(1)[val_indices].float()\n",
    "va_y = y_t.unsqueeze(1)[val_indices].float()\n",
    "\n",
    "def train_loop(epoches, opt, model, loss_fn, trainx, trainy, valx, valy):\n",
    "    for epoch in range(1, epoches+1):\n",
    "        model_train_y = model(trainx)\n",
    "        loss_train = loss_fn(model_train_y, trainy)\n",
    "        \n",
    "        validation_y = model(valx)\n",
    "        loss_validation = loss_fn(validation_y, valy)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss_train.backward()\n",
    "        opt.step()\n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch: %d, train_loss: %f, validation_loss: %f' % (epoch, float(loss_train), float(loss_validation)))\n",
    "    return (model.weight, model.bias)\n",
    "\n",
    "train_loop(5000,\n",
    "          optimizer,\n",
    "          linear_model,\n",
    "          nn.MSELoss(), # dont forget ()\n",
    "          train_x,\n",
    "          train_y,\n",
    "          va_x,\n",
    "          va_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
